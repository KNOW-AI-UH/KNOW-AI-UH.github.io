---
title: Research
---


## Objectives

The objectives are:
1. We pursue a timely and topical application: designing state-of-the-art large language models (LLMs) for detecting global social threats, with clear societal impacts.
2. We explore fundamental problems in LLMs — in the context of this application, namely: model explainability, confidence, and sustainability.


### Detecting Propaganda and Persuasion Techniques in Media

Rapid advances in technology enable easy creation of direct communication channels between information producers and information consumers.
This raises the potential to expose the latter to _deceptive_ content and _mass manipulation_.
Huge audiences are affected online, and major crisis events are subjected to the spread of harmful disinformation and propaganda.

We develop methods for supporting users to analyze the media ecosystem and characterize manipulation attempts.
The goal is to learn about propaganda and disinformation methods, and how they achieves harmful effects.


### Knowledge-Aware and Retrieval-Augmented Language Models

We develop algorithms for multilingual media monitoring that will produce reliable real-time alerts on an open set of event types indicating global threats — including:
- pandemic outbreaks,
  - PULS: in collaboration with [__European Commission's Joint Research Centre__]()
- terrorist attacks,
- socio-political unrest,
- human trafficking, smuggling of arms,
- natural and man-made disasters,
- etc.

LLMs offer unparalleled quality of analysis, but are trained on "legacy" data, which — by construction — is not up-to-date.  We explore how the LLM can draw on external knowledge bases and live media streams — which _are_ up-to-date.
To achieve these goals, we leverage High Performance Computing (HPC) to build and refine LLM methodologies, and a critical mass of international collaboration, with collaborators who bring complementary expertise and resources.



<!--
## Seminar
...
-->
